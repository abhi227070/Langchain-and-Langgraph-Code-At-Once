{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72280e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Output: Paris\n",
      "NVIDIA Output: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Langchain models with appropriate parameters\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
    "nvidia_model = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\", temperature=0.1)\n",
    "\n",
    "# Example usage of the models\n",
    "gemini_response = gemini_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"Gemini Output: {gemini_response.content}\")\n",
    "\n",
    "nvidia_response = nvidia_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"NVIDIA Output: {nvidia_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dfb88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template: What is the capital of France?\n",
      "Chat Prompt Template: messages=[SystemMessage(content='You are a helpful assistant for python coding.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n",
      "Few Shot Prompt Template: Translate the following phrases:\n",
      "\n",
      "\n",
      "Input: Translate 'hello' to Spanish.\n",
      "Output: Hola.\n",
      "\n",
      "Input: Translate 'thank you' to French.\n",
      "Output: Merci.\n",
      "\n",
      "Input: Translate Hello world into Hindi\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Trying different prompt templates in langchain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Prompt Templates\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful assistant for python coding.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ],\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "# Few Shot Prompt Template\n",
    "examples = [\n",
    "    {\"input\": \"Translate 'hello' to Spanish.\", \"output\": \"Hola.\"},\n",
    "    {\"input\": \"Translate 'thank you' to French.\", \"output\": \"Merci.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Translate the following phrases:\\n\",\n",
    "    suffix=\"Input: {query}\\nOutput:\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Example usage of the prompt templates\n",
    "print(f\"Prompt Template: {prompt_template.invoke({'country': 'France'}).text}\")\n",
    "print(f\"Chat Prompt Template: {chat_prompt_template.invoke({'question': 'What is the capital of France?'})}\")\n",
    "print(f\"Few Shot Prompt Template: {few_shot_prompt_template.invoke({'query': 'Translate Hello world into Hindi'}).text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bec16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Combined Output: The capital of India is **New Delhi**.\n",
      "NVIDIA Combined Output: The capital of India is New Delhi.\n",
      "Gemini Chat Output: Of course! Here are several ways to reverse a string in Python, from the most common and \"Pythonic\" to more fundamental approaches.\n",
      "\n",
      "### 1. The Best and Most Pythonic Way: Slicing\n",
      "\n",
      "This is the most concise and widely used method in Python. It uses extended slice syntax.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"hello world\"\n",
      "\n",
      "# The magic happens here: [::-1]\n",
      "reversed_string = original_string[::-1]\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: hello world\n",
      "Reversed: dlrow olleh\n",
      "```\n",
      "\n",
      "**How it works:**\n",
      "The slice syntax is `[start:stop:step]`.\n",
      "*   By leaving `start` and `stop` blank (`:`), we specify the entire string.\n",
      "*   By setting `step` to `-1`, we tell Python to go backward one character at a time.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Using the `reversed()` Function and `join()`\n",
      "\n",
      "This method is also very Pythonic and can be more readable for people unfamiliar with the slice trick.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"Python is fun\"\n",
      "\n",
      "# reversed() returns an iterator that yields characters in reverse order\n",
      "# \"\".join() concatenates them back into a string\n",
      "reversed_string = \"\".join(reversed(original_string))\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: Python is fun\n",
      "Reversed: nuf si nohtyP\n",
      "```\n",
      "\n",
      "**How it works:**\n",
      "1.  `reversed(original_string)` creates an *iterator* that goes through the string from end to start.\n",
      "2.  `\"\".join(...)` takes all the items from the iterator and joins them together into a new string. The `\"\"` at the beginning means there is no separator between the characters.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Using a `for` Loop (The Manual Way)\n",
      "\n",
      "This approach builds the reversed string character by character. It's a great way to understand the underlying logic and is common in other programming languages.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"programming\"\n",
      "reversed_string = \"\"\n",
      "\n",
      "# Iterate through each character in the original string\n",
      "for char in original_string:\n",
      "  # Prepend the character to the new string\n",
      "  reversed_string = char + reversed_string\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: programming\n",
      "Reversed: gnimmargorp\n",
      "```\n",
      "**How it works:**\n",
      "The loop reads the `original_string` from left to right (`p`, then `r`, then `o`...). In each step, it takes the current character (`char`) and places it at the *beginning* of the `reversed_string`.\n",
      "\n",
      "- **Iteration 1:** `char` is 'p'. `reversed_string` becomes 'p' + \"\" -> \"p\"\n",
      "- **Iteration 2:** `char` is 'r'. `reversed_string` becomes 'r' + \"p\" -> \"rp\"\n",
      "- **Iteration 3:** `char` is 'o'. `reversed_string` becomes 'o' + \"rp\" -> \"orp\"\n",
      "- ...and so on.\n",
      "\n",
      "---\n",
      "\n",
      "### Putting it into a Reusable Function\n",
      "\n",
      "It's good practice to wrap this logic in a function. Here's how you would do it using the recommended slicing method.\n",
      "\n",
      "```python\n",
      "def reverse_string(s):\n",
      "  \"\"\"\n",
      "  Reverses a given string using slicing.\n",
      "\n",
      "  Args:\n",
      "    s: The string to be reversed.\n",
      "\n",
      "  Returns:\n",
      "    The reversed string.\n",
      "  \"\"\"\n",
      "  return s[::-1]\n",
      "\n",
      "# --- Example Usage ---\n",
      "my_name = \"Alice\"\n",
      "reversed_name = reverse_string(my_name)\n",
      "print(f\"{my_name} reversed is {reversed_name}\") # Output: Alice reversed is ecilA\n",
      "\n",
      "another_string = \"racecar\"\n",
      "print(f\"{another_string} reversed is {reverse_string(another_string)}\") # Output: racecar reversed is racecar\n",
      "```\n",
      "\n",
      "### Summary: Which Method Should You Use?\n",
      "\n",
      "| Method | Readability | Performance | \"Pythonic\" | When to Use |\n",
      "| :--- | :--- | :--- | :--- | :--- |\n",
      "| **Slicing `[::-1]`** | Good, once you know the trick | **Excellent** | **Most Pythonic** | **Your default choice.** It's fast, concise, and idiomatic. |\n",
      "| **`\"\".join(reversed())`** | **Excellent** | Excellent | Very Pythonic | When you want maximum clarity. It explicitly says \"reversed\" and \"join\". |\n",
      "| **`for` Loop** | Good | Good (but slower for very long strings) | Less Pythonic | When you're learning, or in a coding interview to demonstrate you understand the fundamentals without relying on built-in tricks. |\n",
      "NVIDIA Chat Output: **Reversing a String in Python**\n",
      "================================\n",
      "\n",
      "You can reverse a string in Python using slicing or the built-in `reversed` function. Here are examples of both methods:\n",
      "\n",
      "### Method 1: Using Slicing\n",
      "\n",
      "```python\n",
      "def reverse_string_slicing(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using slicing.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    return input_str[::-1]\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_slicing(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "### Method 2: Using the `reversed` Function\n",
      "\n",
      "```python\n",
      "def reverse_string_reversed(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using the reversed function.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    return \"\".join(reversed(input_str))\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_reversed(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "### Method 3: Using a Loop\n",
      "\n",
      "```python\n",
      "def reverse_string_loop(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using a loop.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    reversed_str = \"\"\n",
      "    for char in input_str:\n",
      "        reversed_str = char + reversed_str\n",
      "    return reversed_str\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_loop(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "Choose the method that best suits your needs. The slicing method is generally the most efficient and Pythonic way to reverse a string.\n",
      "Gemini Few Shot Output: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ (Namaste Duniya)\n",
      "NVIDIA Few Shot Output: Input: Translate 'Hello world' to Hindi.\n",
      "Output: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ (Namaste Duniya)\n"
     ]
    }
   ],
   "source": [
    "# Prompt Templates + Models\n",
    "gemini_prompt_template = prompt_template | gemini_model\n",
    "nvidia_prompt_template = prompt_template | nvidia_model\n",
    "\n",
    "# Chat Prompt Templates + Models\n",
    "gemini_chat_prompt_template = chat_prompt_template | gemini_model\n",
    "nvidia_chat_prompt_template = chat_prompt_template | nvidia_model\n",
    "\n",
    "# Few Shot Prompt Templates + Models\n",
    "gemini_few_shot_prompt_template = few_shot_prompt_template | gemini_model\n",
    "nvidia_few_shot_prompt_template = few_shot_prompt_template | nvidia_model\n",
    "\n",
    "# Example usage of the combined prompt templates and models\n",
    "gemini_combined_response = gemini_prompt_template.invoke({\"country\": \"India\"})\n",
    "nvidia_combined_response = nvidia_prompt_template.invoke({\"country\": \"India\"})\n",
    "print(f\"Gemini Combined Output: {gemini_combined_response.content}\")\n",
    "print(f\"NVIDIA Combined Output: {nvidia_combined_response.content}\")\n",
    "\n",
    "# Example usage of the chat prompt templates with models\n",
    "gemini_chat_response = gemini_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string.\"})\n",
    "nvidia_chat_response = nvidia_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string?\"})\n",
    "print(f\"Gemini Chat Output: {gemini_chat_response.content}\")\n",
    "print(f\"NVIDIA Chat Output: {nvidia_chat_response.content}\")\n",
    "\n",
    "# Example usage of the few shot prompt templates with models\n",
    "gemini_few_shot_response = gemini_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "nvidia_few_shot_response = nvidia_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "print(f\"Gemini Few Shot Output: {gemini_few_shot_response.content}\")\n",
    "print(f\"NVIDIA Few Shot Output: {nvidia_few_shot_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5c53899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a friendly Chatbot.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hii My name is Abhijit.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello there. Nice to meet you, Abhijit!', additional_kwargs={}, response_metadata={}), HumanMessage(content='I am from India and I am learning about Langchain.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Ohh that's great. Langhain is a powerful framework for building applications with language models. What would you like to know about it?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Message PlaceHolders\n",
    "# Define a chat prompt template with messages\n",
    "message_placeholder_prompt = ChatPromptTemplate(\n",
    "    messages= [\n",
    "        (\"system\", \"You are a friendly Chatbot.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{user_query}\"),  \n",
    "    ],\n",
    "    input_variables=[\"country\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Hii My name is Abhijit.\"),\n",
    "    AIMessage(content=\"Hello there. Nice to meet you, Abhijit!\"),\n",
    "    HumanMessage(content=\"I am from India and I am learning about Langchain.\"),\n",
    "    AIMessage(content=\"Ohh that's great. Langhain is a powerful framework for building applications with language models. What would you like to know about it?\"),\n",
    "]\n",
    "\n",
    "final_response = message_placeholder_prompt.invoke({\n",
    "    \"user_query\": \"What is my name?\",\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12df6bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini with Placeholder Memory Output: Your name is Abhijit! You told me when we first started chatting. üòä\n",
      "NVIDIA with Placeholder Memory Output: Your name is Abhijit.\n"
     ]
    }
   ],
   "source": [
    "gemini_with_placeholder_memory = message_placeholder_prompt | gemini_model\n",
    "nvidia_with_placeholder_memory = message_placeholder_prompt | nvidia_model\n",
    "\n",
    "# Example usage of the models with message placeholders\n",
    "print(f\"Gemini with Placeholder Memory Output: {gemini_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")\n",
    "print(f\"NVIDIA with Placeholder Memory Output: {nvidia_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Response: {'name': 'Devika', 'age': 25, 'email': None, 'gender': 'Female'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# With Structured Output\n",
    "# Define a structured output model using Pydantic\n",
    "class UserProfile(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The user's name\")]\n",
    "    age: Optional[Annotated[int, Field(description=\"The user's age, if provided\", ge=0, le=120)]] = None\n",
    "    email: Optional[Annotated[str, Field(description=\"The user's email address\")]] = None\n",
    "    gender: Annotated[Literal[\"Male\", \"Female\"], Field(description=\"The user's gender. If not provided then use your best guess\")]\n",
    "\n",
    "# Create a prompt template for structured output  \n",
    "gemini_structured = gemini_model.with_structured_output(UserProfile)\n",
    "prompt_template_structured = PromptTemplate(\n",
    "    template=\"fetch the user details like name, age and email from the user query: {query}\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Combine the prompt template with the model for structured output\n",
    "structured_chain = prompt_template_structured | gemini_structured\n",
    "structured_response = structured_chain.invoke({\"query\": \"My name is Devika, I am 25 years old.\"})\n",
    "structured_response = structured_response.model_dump()  # Convert to dictionary for easier access\n",
    "\n",
    "# Print the structured response\n",
    "print(f\"Structured Response: {structured_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3a9db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Details: {'name': 'Galaxy S21', 'price': 799.99, 'in_stock': False}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Define a Pydantic model for product details\n",
    "class Product(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The name of the product\")]\n",
    "    price: Annotated[float, Field(description=\"The price of the product in USD\")]\n",
    "    in_stock: Annotated[bool, Field(description=\"Whether the product is in stock\")]\n",
    "    \n",
    "# Define output parsers\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Product)\n",
    "\n",
    "# Define a prompt template for extracting product details\n",
    "prompt_template_product = PromptTemplate(\n",
    "    template=\"\"\"Extract product details from the following text: \n",
    "    {text}\n",
    "    format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt template and the model with the Pydantic parser\n",
    "product_chain = prompt_template_product | gemini_model | pydantic_parser\n",
    "product_response = product_chain.invoke({\n",
    "    \"text\": \"The product is a smartphone named 'Galaxy S21', priced at 799.99 USD, and it is currently out of stock.\"\n",
    "})\n",
    "\n",
    "print(f\"Product Details: {product_response.model_dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71d409c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple String Output: The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# Simple String Output Parser\n",
    "simple_string_parser = StrOutputParser()\n",
    "simple_string_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Combine the simple string template with the model\n",
    "simple_string_chain = simple_string_template | gemini_model | simple_string_parser\n",
    "simple_string_response = simple_string_chain.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(f\"Simple String Output: {simple_string_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076fbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Chain Response: Of course. Here is a brief description of a black hole:\n",
      "\n",
      "A **black hole** is a region in spacetime where gravity is so incredibly strong that nothing‚Äînot even light‚Äîcan escape from it.\n",
      "\n",
      "Here are the key points in brief:\n",
      "\n",
      "*   **How They Form:** Most black holes are formed from the remnants of a massive star that has died in a supernova explosion. The star's core collapses under its own immense gravity, squeezing an enormous amount of mass into an infinitesimally small space.\n",
      "\n",
      "*   **Key Features:**\n",
      "    *   **Singularity:** At the center is a point of infinite density called the singularity, where the known laws of physics break down.\n",
      "    *   **Event Horizon:** This is the boundary around the black hole known as the \"point of no return.\" Once anything crosses the event horizon, it is trapped forever and cannot escape.\n",
      "\n",
      "*   **How They Work:** A black hole's gravity is so powerful because it has a huge amount of mass concentrated in a tiny volume. They are not cosmic vacuum cleaners; they follow the same laws of gravity as other objects. You have to get very close to the event horizon to be pulled in. If our Sun were replaced by a black hole of the same mass, Earth would continue to orbit it just as it does now.\n",
      "\n",
      "*   **Effects on Spacetime:** As predicted by Einstein's theory of general relativity, their immense mass dramatically warps the fabric of spacetime around them. This can cause extreme effects like **time dilation**, where time slows down for an observer closer to the black hole.\n",
      "\n",
      "In essence, a black hole is the ultimate expression of gravity, representing one of the most extreme and mysterious objects in the universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chains\n",
    "\n",
    "# Simple Chain\n",
    "simple_prompt = PromptTemplate(\n",
    "    template = \"Describe about the topic in brief: {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "simple_chain = simple_prompt | gemini_model | parser\n",
    "simple_chain_response = simple_chain.invoke({\"topic\": \"Blackhole\"})\n",
    "print(f\"Simple Chain Response: {simple_chain_response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9423904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§Ö‡§≠‡§ø‡§ú‡•Ä‡§§ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§£‡§æ, RAG ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§î‡§∞ ‡§è‡§ú‡•á‡§Ç‡§ü‡§ø‡§ï ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡•à‡§Ç‡§ó‡§ö‡•á‡§® (Langchain) ‡§î‡§∞ ‡§≤‡•à‡§Ç‡§ó‡§ó‡•ç‡§∞‡§æ‡§´ (Langgraph) ‡§∏‡•Ä‡§ñ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chain\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "translate_prompt = PromptTemplate(\n",
    "    template=\"Translate the following text to Hindi: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "sequential_chain = (\n",
    "    summary_prompt | gemini_model | StrOutputParser()\n",
    ") | (\n",
    "    translate_prompt | gemini_model | StrOutputParser()\n",
    ")\n",
    "\n",
    "sequential_chain_response = sequential_chain.invoke({\n",
    "    \"text\": \"Hello My name is Abhijit Maharana and I am 22 years old. I am learning about Langchain and Langgrpah to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(sequential_chain_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c086d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parallel Chain Response: Summary: Abhijit Maharana, a 22-year-old, is learning Langchain and Langgraph to develop RAG applications and Agentic AI models.\n",
      "\n",
      "Questions:\n",
      "**Question:** What is Abhijit Maharana's age?\n",
      "**Answer:** Abhijit Maharana is 22 years old.\n",
      "\n",
      "**Question:** What technologies is Abhijit Maharana learning to use?\n",
      "**Answer:** Abhijit Maharana is learning Langchain and Langgraph.\n",
      "\n",
      "**Question:** What type of applications is Abhijit Maharana aiming to build using these technologies?\n",
      "**Answer:** Abhijit Maharana is building RAG applications and Agentic AI models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Parallel Chain\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Make 3 questions and answers from the given text: {text}\n",
    "    Format: \n",
    "    Question: \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "merge_prompt = PromptTemplate(\n",
    "    template=\"\"\"Merge the following summary and questions into a single response.\n",
    "    Format should be like this:\n",
    "    Summary: {summary}\n",
    "    Questions: {questions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"summary\", \"questions\"]\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": summary_prompt | gemini_model | StrOutputParser(),\n",
    "        \"questions\": question_prompt | gemini_model | StrOutputParser()\n",
    "    }\n",
    ")\n",
    "\n",
    "merge_chain = merge_prompt | nvidia_model | StrOutputParser()\n",
    "\n",
    "final_parallel_chain = parallel_chain | merge_chain\n",
    "final_parallel_response = final_parallel_chain.invoke({\n",
    "    'text': \"Hello, I am Abhijit Maharana. I am 22 years old and I am learning about Langchain and Langgraph to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(f\"Final Parallel Chain Response: {final_parallel_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dca978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feedback Response: We're so sorry to hear you had a negative experience with our product and that you found the camera unsatisfactory.  We value your feedback and want to understand what specifically didn't meet your expectations. Could you please tell us more about what you found lacking in the camera's performance?  This will help us improve our products in the future.  We'd also like to explore options to help resolve this for you. Please contact us directly at [phone number or email address] so we can assist you further.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "# Conditional Chain\n",
    "class ProductReview(BaseModel):\n",
    "    product_feedback_from_user: Annotated[str, Field(description=\"Feedback provided by the user about the product\")]\n",
    "    sentiment_of_feedback: Annotated[Literal['Positive', 'Negative'], Field(description=\"Sentiment of the feedback provided by the user\")]\n",
    "\n",
    "string_parser = StrOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ProductReview)\n",
    "    \n",
    "sentiment_analysis_prompt = PromptTemplate(\n",
    "    template = \"\"\"Analyze the sentiment of the following feedback: \n",
    "    {feedback}\n",
    "    \n",
    "    Format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "positive_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for positive feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "negative_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for negative feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "sentiment_chain = sentiment_analysis_prompt | gemini_model | pydantic_parser\n",
    "\n",
    "feedback_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment_of_feedback == \"Positive\", positive_feedback_prompt | gemini_model | string_parser),\n",
    "    (lambda x: x.sentiment_of_feedback == \"Negative\", negative_feedback_prompt | gemini_model | string_parser),\n",
    "    RunnableLambda(lambda x: \"404 Error\")\n",
    ")\n",
    "\n",
    "final_feedback_chain = sentiment_chain | feedback_chain\n",
    "final_feedback_response = final_feedback_chain.invoke({\n",
    "    \"feedback\": \"The product very bad. Camera is not good at all!\"\n",
    "})\n",
    "\n",
    "print(f\"Final Feedback Response: {final_feedback_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00f38bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnable Sequence Ouput: The future of AI is incredibly complex and multifaceted, but several key trends are shaping its trajectory:\n",
      "\n",
      "* **Increased Specialization:**  Instead of general-purpose AI, we're likely to see a proliferation of specialized AI systems excelling in narrow domains.  Think AI tailored specifically for medical diagnosis, financial modeling, or climate prediction, rather than one AI attempting to master everything.\n",
      "\n",
      "* **Explainable AI (XAI):**  A major focus is on making AI's decision-making processes more transparent and understandable.  This is crucial for building trust and ensuring accountability, especially in high-stakes applications like healthcare and justice.\n",
      "\n",
      "* **Human-AI Collaboration:** The future isn't about AI replacing humans, but rather augmenting human capabilities.  We'll see more collaborative systems where humans and AI work together, leveraging each other's strengths.\n",
      "\n",
      "* **Ethical Considerations:**  As AI becomes more powerful, ethical concerns around bias, fairness, privacy, and job displacement will become increasingly important.  Developing robust ethical frameworks and regulations will be crucial.\n",
      "\n",
      "* **Hardware Advancements:**  Progress in areas like quantum computing and neuromorphic computing could dramatically accelerate AI development and capabilities, potentially leading to breakthroughs in areas currently considered intractable.\n",
      "\n",
      "* **Generative AI Explosion:**  We're already seeing the rapid advancement of generative AI, capable of creating novel text, images, audio, and even code.  This technology will likely become even more sophisticated and integrated into various aspects of our lives.\n",
      "\n",
      "* **Potential Risks:**  Alongside the benefits, there are potential risks associated with advanced AI, including misuse for malicious purposes, unintended consequences, and the potential for unforeseen societal disruptions.  Careful consideration and proactive mitigation strategies are essential.\n",
      "\n",
      "\n",
      "In short, the future of AI is full of both immense promise and significant challenges.  Its impact will be felt across every aspect of society, and navigating this future responsibly will require careful planning, collaboration, and a commitment to ethical development.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnables\n",
    "# Runnable Sequence\n",
    "runnable_sequence_prompt = PromptTemplate(\n",
    "    template = \"\"\"Tell me something about the topic: {topic}\"\"\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Define String output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define Runnable Sequence Chain\n",
    "runnable_sequence_chain = RunnableSequence(runnable_sequence_prompt, gemini_model, parser)\n",
    "\n",
    "runnable_sequence_response = runnable_sequence_chain.invoke({'topic': 'Future of AI'})\n",
    "print(f\"Runnable Sequence Ouput: {runnable_sequence_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499691b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Here's a summary of the text:\n",
      "\n",
      "A kind girl named Ella, with the help of her fairy godmother, attends a royal ball where she dances with a prince. After midnight, she leaves behind a glass slipper, which the prince uses to find her. Eventually, they get married and live happily ever after.\n",
      "Questions: \n",
      "**Question:** What was the name of the kind girl who lived in the quiet village?\n",
      "**Answer:** The kind girl's name was Ella.\n",
      "\n",
      "**Question:** What magical event allowed Ella to attend the royal ball?\n",
      "**Answer:** Her fairy godmother transformed her rags into a beautiful gown.\n",
      "\n",
      "**Question:** What object did Ella lose at the ball that helped the prince find her?\n",
      "**Answer:** Ella lost her glass slipper on the stairs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnable Parallel\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Make 3 questions and answers from the given text: {text}\n",
    "    Format: \n",
    "    Question: \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "runnable_parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": summary_prompt | nvidia_model | StrOutputParser(),\n",
    "        \"questions\": question_prompt | gemini_model | StrOutputParser()\n",
    "    }\n",
    ")\n",
    "\n",
    "text = \"\"\"Once upon a time, in a quiet village, there lived a kind girl named Ella.\n",
    "One magical night, her fairy godmother appeared and transformed her rags into a beautiful gown so she could attend the royal ball.\n",
    "At the ball, Ella danced with the prince, but at midnight she had to leave, losing her glass slipper on the stairs.\n",
    "The prince searched the kingdom for the girl who fit the slipper, and\n",
    "when he found Ella, they lived happily ever after.\"\"\"\n",
    "\n",
    "# Test chain\n",
    "runnable_parallel_response = runnable_parallel_chain.invoke({'text': text})\n",
    "\n",
    "print(f\"Summary: {runnable_parallel_response['summary']}\")\n",
    "print(f\"Questions: \\n{runnable_parallel_response['questions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Text:\n",
      "Black holes are among the most fascinating and mysterious objects in the universe. Here's a brief overview:\n",
      "\n",
      "**What is a Black Hole?**\n",
      "\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, including light, can escape. It is formed when a massive star collapses in on itself and its gravity becomes so strong that it warps the fabric of spacetime around it.\n",
      "\n",
      "**How are Black Holes Created?**\n",
      "\n",
      "Black holes are created when a massive star runs out of fuel and dies. If the star is massive enough (about 3-4 times the size of the sun), its gravity will collapse the star in on itself, causing a supernova explosion. If the star is even more massive (about 10-20 times the size of the sun), the collapse will create a black hole.\n",
      "\n",
      "**Characteristics of Black Holes**\n",
      "\n",
      "Black holes have several characteristics that make them unique:\n",
      "\n",
      "1. **Event Horizon**: The point of no return around a black hole is called the event horizon. Once something crosses the event horizon, it is trapped by the black hole's gravity and cannot escape.\n",
      "2. **Singularity**: The center of a black hole is called a singularity, where the density and gravity are infinite.\n",
      "3. **Gravitational Pull**: Black holes have an incredibly strong gravitational pull, which is so strong that not even light can escape.\n",
      "4. **No Emission**: Black holes do not emit any radiation or light, making them invisible to our telescopes.\n",
      "\n",
      "**Types of Black Holes**\n",
      "\n",
      "There are four types of black holes, each with different properties:\n",
      "\n",
      "1. **Stellar Black Holes**: Formed from the collapse of individual stars.\n",
      "2. **Supermassive Black Holes**: Found at the centers of galaxies, these black holes can have masses millions or even billions of times that of the sun.\n",
      "3. **Intermediate-Mass Black Holes**: Black holes with masses that fall between those of stellar and supermassive black holes.\n",
      "4. **Primordial Black Holes**: Hypothetical black holes that may have formed in the early universe before the first stars formed.\n",
      "\n",
      "**Observational Evidence for Black Holes**\n",
      "\n",
      "While black holes themselves are invisible, their presence can be inferred by observing the effects they have on the surrounding environment. Some of the observational evidence for black holes includes:\n",
      "\n",
      "1. **X-rays and Gamma Rays**: Telescopes can detect X-rays and gamma rays emitted by hot gas swirling around black holes.\n",
      "2. **Radio Waves**: Radio telescopes can detect radio waves emitted by matter as it spirals into a black hole.\n",
      "3. **Star Motions**: Astronomers can observe the motions of stars near a suspected black hole to determine if they are being affected by its gravity.\n",
      "\n",
      "**The Mysteries of Black Holes**\n",
      "\n",
      "Despite the significant progress made in understanding black holes, there is still much to be learned. Some of the biggest mysteries surrounding black holes include:\n",
      "\n",
      "1. **Information Paradox**: What happens to the information contained in matter that falls into a black hole?\n",
      "2. **Singularity**: How can the density and gravity at the center of a black hole be explained?\n",
      "3. **Black Hole Entropy**: How can the entropy (a measure of disorder or randomness) of a black hole be understood?\n",
      "\n",
      "Black holes continue to fascinate scientists and the public alike, and ongoing research is helping to unravel the mysteries of these enigmatic objects.\n",
      "Summary Text: \n",
      "Black holes are regions of spacetime with gravity so strong that nothing, not even light, can escape.  They form when massive stars (3-4 times the sun's mass or greater) collapse at the end of their lives, sometimes resulting in a supernova.  Black holes are characterized by an event horizon (the point of no return), a singularity (a point of infinite density at the center), immense gravitational pull, and the absence of light emission.  Four types exist: stellar, supermassive (found at galactic centers), intermediate-mass, and hypothetical primordial black holes.  While invisible, their presence is detected through observations of X-rays, gamma rays, radio waves emitted by surrounding matter, and the gravitational influence on nearby stars.  Despite significant understanding, mysteries remain, including the information paradox, the nature of the singularity, and black hole entropy.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnable Passthrough\n",
    "topic_prompt = PromptTemplate(\n",
    "    template = \"\"\"Tell me something about the topic: {topic}\"\"\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template= \"\"\"\n",
    "    Summarize the following text:\n",
    "    {text}\n",
    "    \"\"\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "# Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Topic Chain\n",
    "topic_chain = topic_prompt | nvidia_model | parser\n",
    "\n",
    "# Runnable Passthrough Chain\n",
    "runnable_passthrough_chain = RunnableParallel({\n",
    "    'topic_text': RunnablePassthrough(),\n",
    "    'summary_text': summary_prompt | gemini_model | parser\n",
    "})\n",
    "\n",
    "# Final Chain\n",
    "final_chain = topic_chain | runnable_passthrough_chain\n",
    "\n",
    "runnable_passthrough_response = final_chain.invoke({'topic': 'blackhole'})\n",
    "print(f\"Topic Text:\\n{runnable_passthrough_response['topic_text']}\")\n",
    "print(f\"Summary Text: \\n{runnable_passthrough_response['summary_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83f554ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 10\n",
      "Joke: \n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Runnable Lambda \n",
    "joke_prompt = PromptTemplate(\n",
    "    template = \"Tell me a small joke about the topic: {topic}\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Define Function for lambda\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Joke Chain\n",
    "joke_chain = joke_prompt | gemini_model | parser\n",
    "\n",
    "# Lambda Chain\n",
    "lambda_chain = RunnableParallel({\n",
    "    'word_count': RunnableLambda(count_words),\n",
    "    'joke': RunnablePassthrough()\n",
    "}\n",
    ")\n",
    "\n",
    "# Final Chain\n",
    "final_runnable_lambda_chain = joke_chain | lambda_chain\n",
    "\n",
    "runnable_lambda_response = final_runnable_lambda_chain.invoke({'topic': 'Programming'})\n",
    "print(f\"Word Count: {runnable_lambda_response['word_count']}\")\n",
    "print(f\"Joke: \\n{runnable_lambda_response['joke']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8c77ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhijeet, a 22-year-old AI/ML engineer, builds RAG-based applications and agentic AI workflows for industrial use.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template= \"Summarize the following text: \\n {text}\",\n",
    "    input_variables= ['text']\n",
    ")\n",
    "\n",
    "branched_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split()) > 20, summary_prompt | gemini_model | StrOutputParser()),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "\n",
    "response = branched_chain.invoke(\"Hello My name is Abhijeet. I am 22 years old and I am doing AI ML Engineering. I build many rag based applications and Agentic AI workflows for Industry.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5fae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
