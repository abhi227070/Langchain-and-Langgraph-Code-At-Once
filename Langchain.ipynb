{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72280e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Langchain models with appropriate parameters\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
    "nvidia_model = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\", temperature=0.1)\n",
    "\n",
    "# Example usage of the models\n",
    "gemini_response = gemini_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"Gemini Output: {gemini_response.content}\")\n",
    "\n",
    "nvidia_response = nvidia_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"NVIDIA Output: {nvidia_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying different prompt templates in langchain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Prompt Templates\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful assistant for python coding.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ],\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "# Few Shot Prompt Template\n",
    "examples = [\n",
    "    {\"input\": \"Translate 'hello' to Spanish.\", \"output\": \"Hola.\"},\n",
    "    {\"input\": \"Translate 'thank you' to French.\", \"output\": \"Merci.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Translate the following phrases:\\n\",\n",
    "    suffix=\"Input: {query}\\nOutput:\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Example usage of the prompt templates\n",
    "print(f\"Prompt Template: {prompt_template.invoke({'country': 'France'}).text}\")\n",
    "print(f\"Chat Prompt Template: {chat_prompt_template.invoke({'question': 'What is the capital of France?'})}\")\n",
    "print(f\"Few Shot Prompt Template: {few_shot_prompt_template.invoke({'query': 'Translate Hello world into Hindi'}).text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates + Models\n",
    "gemini_prompt_template = prompt_template | gemini_model\n",
    "nvidia_prompt_template = prompt_template | nvidia_model\n",
    "\n",
    "# Chat Prompt Templates + Models\n",
    "gemini_chat_prompt_template = chat_prompt_template | gemini_model\n",
    "nvidia_chat_prompt_template = chat_prompt_template | nvidia_model\n",
    "\n",
    "# Few Shot Prompt Templates + Models\n",
    "gemini_few_shot_prompt_template = few_shot_prompt_template | gemini_model\n",
    "nvidia_few_shot_prompt_template = few_shot_prompt_template | nvidia_model\n",
    "\n",
    "# Example usage of the combined prompt templates and models\n",
    "gemini_combined_response = gemini_prompt_template.invoke({\"country\": \"India\"})\n",
    "nvidia_combined_response = nvidia_prompt_template.invoke({\"country\": \"India\"})\n",
    "print(f\"Gemini Combined Output: {gemini_combined_response.content}\")\n",
    "print(f\"NVIDIA Combined Output: {nvidia_combined_response.content}\")\n",
    "\n",
    "# Example usage of the chat prompt templates with models\n",
    "gemini_chat_response = gemini_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string.\"})\n",
    "nvidia_chat_response = nvidia_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string?\"})\n",
    "print(f\"Gemini Chat Output: {gemini_chat_response.content}\")\n",
    "print(f\"NVIDIA Chat Output: {nvidia_chat_response.content}\")\n",
    "\n",
    "# Example usage of the few shot prompt templates with models\n",
    "gemini_few_shot_response = gemini_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "nvidia_few_shot_response = nvidia_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "print(f\"Gemini Few Shot Output: {gemini_few_shot_response.content}\")\n",
    "print(f\"NVIDIA Few Shot Output: {nvidia_few_shot_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c53899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Message PlaceHolders\n",
    "# Define a chat prompt template with messages\n",
    "message_placeholder_prompt = ChatPromptTemplate(\n",
    "    messages= [\n",
    "        (\"system\", \"You are a friendly Chatbot.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{user_query}\"),  \n",
    "    ],\n",
    "    input_variables=[\"country\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Hii My name is Abhijit.\"),\n",
    "    AIMessage(content=\"Hello there. Nice to meet you, Abhijit!\"),\n",
    "    HumanMessage(content=\"I am from India and I am learning about Langchain.\"),\n",
    "    AIMessage(content=\"Ohh that's great. Langhain is a powerful framework for building applications with language models. What would you like to know about it?\"),\n",
    "]\n",
    "\n",
    "final_response = message_placeholder_prompt.invoke({\n",
    "    \"user_query\": \"What is my name?\",\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_with_placeholder_memory = message_placeholder_prompt | gemini_model\n",
    "nvidia_with_placeholder_memory = message_placeholder_prompt | nvidia_model\n",
    "\n",
    "# Example usage of the models with message placeholders\n",
    "print(f\"Gemini with Placeholder Memory Output: {gemini_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")\n",
    "print(f\"NVIDIA with Placeholder Memory Output: {nvidia_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# With Structured Output\n",
    "# Define a structured output model using Pydantic\n",
    "class UserProfile(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The user's name\")]\n",
    "    age: Optional[Annotated[int, Field(description=\"The user's age, if provided\", ge=0, le=120)]] = None\n",
    "    email: Optional[Annotated[str, Field(description=\"The user's email address\")]] = None\n",
    "    gender: Annotated[Literal[\"Male\", \"Female\"], Field(description=\"The user's gender. If not provided then use your best guess\")]\n",
    "\n",
    "# Create a prompt template for structured output  \n",
    "gemini_structured = gemini_model.with_structured_output(UserProfile)\n",
    "prompt_template_structured = PromptTemplate(\n",
    "    template=\"fetch the user details like name, age and email from the user query: {query}\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Combine the prompt template with the model for structured output\n",
    "structured_chain = prompt_template_structured | gemini_structured\n",
    "structured_response = structured_chain.invoke({\"query\": \"My name is Devika, I am 25 years old.\"})\n",
    "structured_response = structured_response.model_dump()  # Convert to dictionary for easier access\n",
    "\n",
    "# Print the structured response\n",
    "print(f\"Structured Response: {structured_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a9db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Define a Pydantic model for product details\n",
    "class Product(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The name of the product\")]\n",
    "    price: Annotated[float, Field(description=\"The price of the product in USD\")]\n",
    "    in_stock: Annotated[bool, Field(description=\"Whether the product is in stock\")]\n",
    "    \n",
    "# Define output parsers\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Product)\n",
    "\n",
    "# Define a prompt template for extracting product details\n",
    "prompt_template_product = PromptTemplate(\n",
    "    template=\"\"\"Extract product details from the following text: \n",
    "    {text}\n",
    "    format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt template and the model with the Pydantic parser\n",
    "product_chain = prompt_template_product | gemini_model | pydantic_parser\n",
    "product_response = product_chain.invoke({\n",
    "    \"text\": \"The product is a smartphone named 'Galaxy S21', priced at 799.99 USD, and it is currently out of stock.\"\n",
    "})\n",
    "\n",
    "print(f\"Product Details: {product_response.model_dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d409c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple String Output Parser\n",
    "simple_string_parser = StrOutputParser()\n",
    "simple_string_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Combine the simple string template with the model\n",
    "simple_string_chain = simple_string_template | gemini_model | simple_string_parser\n",
    "simple_string_response = simple_string_chain.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(f\"Simple String Output: {simple_string_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chains\n",
    "\n",
    "# Simple Chain\n",
    "simple_prompt = PromptTemplate(\n",
    "    template = \"Describe about the topic in brief: {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "simple_chain = simple_prompt | gemini_model | parser\n",
    "simple_chain_response = simple_chain.invoke({\"topic\": \"Blackhole\"})\n",
    "print(f\"Simple Chain Response: {simple_chain_response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9423904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chain\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "translate_prompt = PromptTemplate(\n",
    "    template=\"Translate the following text to Hindi: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "sequential_chain = (\n",
    "    summary_prompt | gemini_model | StrOutputParser()\n",
    ") | (\n",
    "    translate_prompt | gemini_model | StrOutputParser()\n",
    ")\n",
    "\n",
    "sequential_chain_response = sequential_chain.invoke({\n",
    "    \"text\": \"Hello My name is Abhijit Maharana and I am 22 years old. I am learning about Langchain and Langgrpah to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(sequential_chain_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Parallel Chain\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Make 3 questions and answers from the given text: {text}\n",
    "    Format: \n",
    "    Question: \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "merge_prompt = PromptTemplate(\n",
    "    template=\"\"\"Merge the following summary and questions into a single response.\n",
    "    Format should be like this:\n",
    "    Summary: {summary}\n",
    "    Questions: {questions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"summary\", \"questions\"]\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": summary_prompt | gemini_model | StrOutputParser(),\n",
    "        \"questions\": question_prompt | gemini_model | StrOutputParser()\n",
    "    }\n",
    ")\n",
    "\n",
    "merge_chain = merge_prompt | nvidia_model | StrOutputParser()\n",
    "\n",
    "final_parallel_chain = parallel_chain | merge_chain\n",
    "final_parallel_response = final_parallel_chain.invoke({\n",
    "    'text': \"Hello, I am Abhijit Maharana. I am 22 years old and I am learning about Langchain and Langgraph to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(f\"Final Parallel Chain Response: {final_parallel_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dca978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "# Conditional Chain\n",
    "class ProductReview(BaseModel):\n",
    "    product_feedback_from_user: Annotated[str, Field(description=\"Feedback provided by the user about the product\")]\n",
    "    sentiment_of_feedback: Annotated[Literal['Positive', 'Negative'], Field(description=\"Sentiment of the feedback provided by the user\")]\n",
    "\n",
    "string_parser = StrOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ProductReview)\n",
    "    \n",
    "sentiment_analysis_prompt = PromptTemplate(\n",
    "    template = \"\"\"Analyze the sentiment of the following feedback: \n",
    "    {feedback}\n",
    "    \n",
    "    Format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "positive_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for positive feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "negative_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for negative feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "sentiment_chain = sentiment_analysis_prompt | gemini_model | pydantic_parser\n",
    "\n",
    "feedback_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment_of_feedback == \"Positive\", positive_feedback_prompt | gemini_model | string_parser),\n",
    "    (lambda x: x.sentiment_of_feedback == \"Negative\", negative_feedback_prompt | gemini_model | string_parser),\n",
    "    RunnableLambda(lambda x: \"404 Error\")\n",
    ")\n",
    "\n",
    "final_feedback_chain = sentiment_chain | feedback_chain\n",
    "final_feedback_response = final_feedback_chain.invoke({\n",
    "    \"feedback\": \"The product very bad. Camera is not good at all!\"\n",
    "})\n",
    "\n",
    "print(f\"Final Feedback Response: {final_feedback_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnables\n",
    "# Runnable Sequence\n",
    "runnable_sequence_prompt = PromptTemplate(\n",
    "    template = \"\"\"Tell me something about the topic: {topic}\"\"\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Define String output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define Runnable Sequence Chain\n",
    "runnable_sequence_chain = RunnableSequence(runnable_sequence_prompt, gemini_model, parser)\n",
    "\n",
    "runnable_sequence_response = runnable_sequence_chain.invoke({'topic': 'Future of AI'})\n",
    "print(f\"Runnable Sequence Ouput: {runnable_sequence_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnable Parallel\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Make 3 questions and answers from the given text: {text}\n",
    "    Format: \n",
    "    Question: \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "runnable_parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": summary_prompt | nvidia_model | StrOutputParser(),\n",
    "        \"questions\": question_prompt | gemini_model | StrOutputParser()\n",
    "    }\n",
    ")\n",
    "\n",
    "text = \"\"\"Once upon a time, in a quiet village, there lived a kind girl named Ella.\n",
    "One magical night, her fairy godmother appeared and transformed her rags into a beautiful gown so she could attend the royal ball.\n",
    "At the ball, Ella danced with the prince, but at midnight she had to leave, losing her glass slipper on the stairs.\n",
    "The prince searched the kingdom for the girl who fit the slipper, and\n",
    "when he found Ella, they lived happily ever after.\"\"\"\n",
    "\n",
    "# Test chain\n",
    "runnable_parallel_response = runnable_parallel_chain.invoke({'text': text})\n",
    "\n",
    "print(f\"Summary: {runnable_parallel_response['summary']}\")\n",
    "print(f\"Questions: \\n{runnable_parallel_response['questions']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Runnable Passthrough\n",
    "topic_prompt = PromptTemplate(\n",
    "    template = \"\"\"Tell me something about the topic: {topic}\"\"\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template= \"\"\"\n",
    "    Summarize the following text:\n",
    "    {text}\n",
    "    \"\"\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "# Parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Topic Chain\n",
    "topic_chain = topic_prompt | nvidia_model | parser\n",
    "\n",
    "# Runnable Passthrough Chain\n",
    "runnable_passthrough_chain = RunnableParallel({\n",
    "    'topic_text': RunnablePassthrough(),\n",
    "    'summary_text': summary_prompt | gemini_model | parser\n",
    "})\n",
    "\n",
    "# Final Chain\n",
    "final_chain = topic_chain | runnable_passthrough_chain\n",
    "\n",
    "runnable_passthrough_response = final_chain.invoke({'topic': 'blackhole'})\n",
    "print(f\"Topic Text:\\n{runnable_passthrough_response['topic_text']}\")\n",
    "print(f\"Summary Text: \\n{runnable_passthrough_response['summary_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f554ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Runnable Lambda \n",
    "joke_prompt = PromptTemplate(\n",
    "    template = \"Tell me a small joke about the topic: {topic}\",\n",
    "    input_variables= ['topic']\n",
    ")\n",
    "\n",
    "# Define Function for lambda\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Joke Chain\n",
    "joke_chain = joke_prompt | gemini_model | parser\n",
    "\n",
    "# Lambda Chain\n",
    "lambda_chain = RunnableParallel({\n",
    "    'word_count': RunnableLambda(count_words),\n",
    "    'joke': RunnablePassthrough()\n",
    "}\n",
    ")\n",
    "\n",
    "# Final Chain\n",
    "final_runnable_lambda_chain = joke_chain | lambda_chain\n",
    "\n",
    "runnable_lambda_response = final_runnable_lambda_chain.invoke({'topic': 'Programming'})\n",
    "print(f\"Word Count: {runnable_lambda_response['word_count']}\")\n",
    "print(f\"Joke: \\n{runnable_lambda_response['joke']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Required Prompts\n",
    "summary_prompt = PromptTemplate(\n",
    "    template= \"Summarize the following text: \\n {text}\",\n",
    "    input_variables= ['text']\n",
    ")\n",
    "\n",
    "branched_chain = RunnableBranch(\n",
    "    (lambda x: len(x.split()) > 20, summary_prompt | gemini_model | StrOutputParser()),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "\n",
    "response = branched_chain.invoke(\"Hello My name is Abhijeet. I am 22 years old and I am doing AI ML Engineering. I build many rag based applications and Agentic AI workflows for Industry.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa5fae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
