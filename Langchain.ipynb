{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72280e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Output: Paris\n",
      "NVIDIA Output: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Langchain models with appropriate parameters\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
    "nvidia_model = ChatNVIDIA(model=\"meta/llama-3.2-3b-instruct\", temperature=0.1)\n",
    "\n",
    "# Example usage of the models\n",
    "gemini_response = gemini_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"Gemini Output: {gemini_response.content}\")\n",
    "\n",
    "nvidia_response = nvidia_model.invoke(\"What is the capital of France?\")\n",
    "print(f\"NVIDIA Output: {nvidia_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dfb88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template: What is the capital of France?\n",
      "Chat Prompt Template: messages=[SystemMessage(content='You are a helpful assistant for python coding.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={})]\n",
      "Few Shot Prompt Template: Translate the following phrases:\n",
      "\n",
      "\n",
      "Input: Translate 'hello' to Spanish.\n",
      "Output: Hola.\n",
      "\n",
      "Input: Translate 'thank you' to French.\n",
      "Output: Merci.\n",
      "\n",
      "Input: Translate Hello world into Hindi\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Trying different prompt templates in langchain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# Prompt Templates\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "chat_prompt_template = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful assistant for python coding.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ],\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "# Few Shot Prompt Template\n",
    "examples = [\n",
    "    {\"input\": \"Translate 'hello' to Spanish.\", \"output\": \"Hola.\"},\n",
    "    {\"input\": \"Translate 'thank you' to French.\", \"output\": \"Merci.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Translate the following phrases:\\n\",\n",
    "    suffix=\"Input: {query}\\nOutput:\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Example usage of the prompt templates\n",
    "print(f\"Prompt Template: {prompt_template.invoke({'country': 'France'}).text}\")\n",
    "print(f\"Chat Prompt Template: {chat_prompt_template.invoke({'question': 'What is the capital of France?'})}\")\n",
    "print(f\"Few Shot Prompt Template: {few_shot_prompt_template.invoke({'query': 'Translate Hello world into Hindi'}).text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bec16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Combined Output: The capital of India is **New Delhi**.\n",
      "NVIDIA Combined Output: The capital of India is New Delhi.\n",
      "Gemini Chat Output: Of course! Here are several ways to reverse a string in Python, from the most common and \"Pythonic\" to more fundamental approaches.\n",
      "\n",
      "### 1. The Best and Most Pythonic Way: Slicing\n",
      "\n",
      "This is the most concise and widely used method in Python. It uses extended slice syntax.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"hello world\"\n",
      "\n",
      "# The magic happens here: [::-1]\n",
      "reversed_string = original_string[::-1]\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: hello world\n",
      "Reversed: dlrow olleh\n",
      "```\n",
      "\n",
      "**How it works:**\n",
      "The slice syntax is `[start:stop:step]`.\n",
      "*   By leaving `start` and `stop` blank (`:`), we specify the entire string.\n",
      "*   By setting `step` to `-1`, we tell Python to go backward one character at a time.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Using the `reversed()` Function and `join()`\n",
      "\n",
      "This method is also very Pythonic and can be more readable for people unfamiliar with the slice trick.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"Python is fun\"\n",
      "\n",
      "# reversed() returns an iterator that yields characters in reverse order\n",
      "# \"\".join() concatenates them back into a string\n",
      "reversed_string = \"\".join(reversed(original_string))\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: Python is fun\n",
      "Reversed: nuf si nohtyP\n",
      "```\n",
      "\n",
      "**How it works:**\n",
      "1.  `reversed(original_string)` creates an *iterator* that goes through the string from end to start.\n",
      "2.  `\"\".join(...)` takes all the items from the iterator and joins them together into a new string. The `\"\"` at the beginning means there is no separator between the characters.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Using a `for` Loop (The Manual Way)\n",
      "\n",
      "This approach builds the reversed string character by character. It's a great way to understand the underlying logic and is common in other programming languages.\n",
      "\n",
      "**Code:**\n",
      "```python\n",
      "original_string = \"programming\"\n",
      "reversed_string = \"\"\n",
      "\n",
      "# Iterate through each character in the original string\n",
      "for char in original_string:\n",
      "  # Prepend the character to the new string\n",
      "  reversed_string = char + reversed_string\n",
      "\n",
      "print(f\"Original: {original_string}\")\n",
      "print(f\"Reversed: {reversed_string}\")\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "Original: programming\n",
      "Reversed: gnimmargorp\n",
      "```\n",
      "**How it works:**\n",
      "The loop reads the `original_string` from left to right (`p`, then `r`, then `o`...). In each step, it takes the current character (`char`) and places it at the *beginning* of the `reversed_string`.\n",
      "\n",
      "- **Iteration 1:** `char` is 'p'. `reversed_string` becomes 'p' + \"\" -> \"p\"\n",
      "- **Iteration 2:** `char` is 'r'. `reversed_string` becomes 'r' + \"p\" -> \"rp\"\n",
      "- **Iteration 3:** `char` is 'o'. `reversed_string` becomes 'o' + \"rp\" -> \"orp\"\n",
      "- ...and so on.\n",
      "\n",
      "---\n",
      "\n",
      "### Putting it into a Reusable Function\n",
      "\n",
      "It's good practice to wrap this logic in a function. Here's how you would do it using the recommended slicing method.\n",
      "\n",
      "```python\n",
      "def reverse_string(s):\n",
      "  \"\"\"\n",
      "  Reverses a given string using slicing.\n",
      "\n",
      "  Args:\n",
      "    s: The string to be reversed.\n",
      "\n",
      "  Returns:\n",
      "    The reversed string.\n",
      "  \"\"\"\n",
      "  return s[::-1]\n",
      "\n",
      "# --- Example Usage ---\n",
      "my_name = \"Alice\"\n",
      "reversed_name = reverse_string(my_name)\n",
      "print(f\"{my_name} reversed is {reversed_name}\") # Output: Alice reversed is ecilA\n",
      "\n",
      "another_string = \"racecar\"\n",
      "print(f\"{another_string} reversed is {reverse_string(another_string)}\") # Output: racecar reversed is racecar\n",
      "```\n",
      "\n",
      "### Summary: Which Method Should You Use?\n",
      "\n",
      "| Method | Readability | Performance | \"Pythonic\" | When to Use |\n",
      "| :--- | :--- | :--- | :--- | :--- |\n",
      "| **Slicing `[::-1]`** | Good, once you know the trick | **Excellent** | **Most Pythonic** | **Your default choice.** It's fast, concise, and idiomatic. |\n",
      "| **`\"\".join(reversed())`** | **Excellent** | Excellent | Very Pythonic | When you want maximum clarity. It explicitly says \"reversed\" and \"join\". |\n",
      "| **`for` Loop** | Good | Good (but slower for very long strings) | Less Pythonic | When you're learning, or in a coding interview to demonstrate you understand the fundamentals without relying on built-in tricks. |\n",
      "NVIDIA Chat Output: **Reversing a String in Python**\n",
      "================================\n",
      "\n",
      "You can reverse a string in Python using slicing or the built-in `reversed` function. Here are examples of both methods:\n",
      "\n",
      "### Method 1: Using Slicing\n",
      "\n",
      "```python\n",
      "def reverse_string_slicing(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using slicing.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    return input_str[::-1]\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_slicing(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "### Method 2: Using the `reversed` Function\n",
      "\n",
      "```python\n",
      "def reverse_string_reversed(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using the reversed function.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    return \"\".join(reversed(input_str))\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_reversed(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "### Method 3: Using a Loop\n",
      "\n",
      "```python\n",
      "def reverse_string_loop(input_str):\n",
      "    \"\"\"\n",
      "    Reverses a string using a loop.\n",
      "\n",
      "    Args:\n",
      "        input_str (str): The input string to be reversed.\n",
      "\n",
      "    Returns:\n",
      "        str: The reversed string.\n",
      "    \"\"\"\n",
      "    reversed_str = \"\"\n",
      "    for char in input_str:\n",
      "        reversed_str = char + reversed_str\n",
      "    return reversed_str\n",
      "\n",
      "# Example usage:\n",
      "input_str = \"Hello, World!\"\n",
      "reversed_str = reverse_string_loop(input_str)\n",
      "print(reversed_str)  # Output: \"!dlroW ,olleH\"\n",
      "```\n",
      "\n",
      "Choose the method that best suits your needs. The slicing method is generally the most efficient and Pythonic way to reverse a string.\n",
      "Gemini Few Shot Output: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ (Namaste Duniya)\n",
      "NVIDIA Few Shot Output: Input: Translate 'Hello world' to Hindi.\n",
      "Output: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ (Namaste Duniya)\n"
     ]
    }
   ],
   "source": [
    "# Prompt Templates + Models\n",
    "gemini_prompt_template = prompt_template | gemini_model\n",
    "nvidia_prompt_template = prompt_template | nvidia_model\n",
    "\n",
    "# Chat Prompt Templates + Models\n",
    "gemini_chat_prompt_template = chat_prompt_template | gemini_model\n",
    "nvidia_chat_prompt_template = chat_prompt_template | nvidia_model\n",
    "\n",
    "# Few Shot Prompt Templates + Models\n",
    "gemini_few_shot_prompt_template = few_shot_prompt_template | gemini_model\n",
    "nvidia_few_shot_prompt_template = few_shot_prompt_template | nvidia_model\n",
    "\n",
    "# Example usage of the combined prompt templates and models\n",
    "gemini_combined_response = gemini_prompt_template.invoke({\"country\": \"India\"})\n",
    "nvidia_combined_response = nvidia_prompt_template.invoke({\"country\": \"India\"})\n",
    "print(f\"Gemini Combined Output: {gemini_combined_response.content}\")\n",
    "print(f\"NVIDIA Combined Output: {nvidia_combined_response.content}\")\n",
    "\n",
    "# Example usage of the chat prompt templates with models\n",
    "gemini_chat_response = gemini_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string.\"})\n",
    "nvidia_chat_response = nvidia_chat_prompt_template.invoke({\"question\": \"Write python code to reverse a string?\"})\n",
    "print(f\"Gemini Chat Output: {gemini_chat_response.content}\")\n",
    "print(f\"NVIDIA Chat Output: {nvidia_chat_response.content}\")\n",
    "\n",
    "# Example usage of the few shot prompt templates with models\n",
    "gemini_few_shot_response = gemini_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "nvidia_few_shot_response = nvidia_few_shot_prompt_template.invoke({\"query\": \"Translate Hello world into Hindi\"})\n",
    "print(f\"Gemini Few Shot Output: {gemini_few_shot_response.content}\")\n",
    "print(f\"NVIDIA Few Shot Output: {nvidia_few_shot_response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5c53899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a friendly Chatbot.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hii My name is Abhijit.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello there. Nice to meet you, Abhijit!', additional_kwargs={}, response_metadata={}), HumanMessage(content='I am from India and I am learning about Langchain.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Ohh that's great. Langhain is a powerful framework for building applications with language models. What would you like to know about it?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Message PlaceHolders\n",
    "# Define a chat prompt template with messages\n",
    "message_placeholder_prompt = ChatPromptTemplate(\n",
    "    messages= [\n",
    "        (\"system\", \"You are a friendly Chatbot.\"),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{user_query}\"),  \n",
    "    ],\n",
    "    input_variables=[\"country\", \"chat_history\"]\n",
    ")\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Hii My name is Abhijit.\"),\n",
    "    AIMessage(content=\"Hello there. Nice to meet you, Abhijit!\"),\n",
    "    HumanMessage(content=\"I am from India and I am learning about Langchain.\"),\n",
    "    AIMessage(content=\"Ohh that's great. Langhain is a powerful framework for building applications with language models. What would you like to know about it?\"),\n",
    "]\n",
    "\n",
    "final_response = message_placeholder_prompt.invoke({\n",
    "    \"user_query\": \"What is my name?\",\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12df6bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini with Placeholder Memory Output: Your name is Abhijit! You told me when we first started chatting. üòä\n",
      "NVIDIA with Placeholder Memory Output: Your name is Abhijit.\n"
     ]
    }
   ],
   "source": [
    "gemini_with_placeholder_memory = message_placeholder_prompt | gemini_model\n",
    "nvidia_with_placeholder_memory = message_placeholder_prompt | nvidia_model\n",
    "\n",
    "# Example usage of the models with message placeholders\n",
    "print(f\"Gemini with Placeholder Memory Output: {gemini_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")\n",
    "print(f\"NVIDIA with Placeholder Memory Output: {nvidia_with_placeholder_memory.invoke({'chat_history': chat_history, 'user_query': 'What is my name?'}).content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Response: {'name': 'Devika', 'age': 25, 'email': None, 'gender': 'Female'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# With Structured Output\n",
    "# Define a structured output model using Pydantic\n",
    "class UserProfile(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The user's name\")]\n",
    "    age: Optional[Annotated[int, Field(description=\"The user's age, if provided\", ge=0, le=120)]] = None\n",
    "    email: Optional[Annotated[str, Field(description=\"The user's email address\")]] = None\n",
    "    gender: Annotated[Literal[\"Male\", \"Female\"], Field(description=\"The user's gender. If not provided then use your best guess\")]\n",
    "\n",
    "# Create a prompt template for structured output  \n",
    "gemini_structured = gemini_model.with_structured_output(UserProfile)\n",
    "prompt_template_structured = PromptTemplate(\n",
    "    template=\"fetch the user details like name, age and email from the user query: {query}\",\n",
    "    input_variables=[\"query\"]\n",
    ")\n",
    "\n",
    "# Combine the prompt template with the model for structured output\n",
    "structured_chain = prompt_template_structured | gemini_structured\n",
    "structured_response = structured_chain.invoke({\"query\": \"My name is Devika, I am 25 years old.\"})\n",
    "structured_response = structured_response.model_dump()  # Convert to dictionary for easier access\n",
    "\n",
    "# Print the structured response\n",
    "print(f\"Structured Response: {structured_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3a9db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Details: {'name': 'Galaxy S21', 'price': 799.99, 'in_stock': False}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Define a Pydantic model for product details\n",
    "class Product(BaseModel):\n",
    "    name: Annotated[str, Field(description=\"The name of the product\")]\n",
    "    price: Annotated[float, Field(description=\"The price of the product in USD\")]\n",
    "    in_stock: Annotated[bool, Field(description=\"Whether the product is in stock\")]\n",
    "    \n",
    "# Define output parsers\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=Product)\n",
    "\n",
    "# Define a prompt template for extracting product details\n",
    "prompt_template_product = PromptTemplate(\n",
    "    template=\"\"\"Extract product details from the following text: \n",
    "    {text}\n",
    "    format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Create a chain that combines the prompt template and the model with the Pydantic parser\n",
    "product_chain = prompt_template_product | gemini_model | pydantic_parser\n",
    "product_response = product_chain.invoke({\n",
    "    \"text\": \"The product is a smartphone named 'Galaxy S21', priced at 799.99 USD, and it is currently out of stock.\"\n",
    "})\n",
    "\n",
    "print(f\"Product Details: {product_response.model_dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71d409c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple String Output: The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "# Simple String Output Parser\n",
    "simple_string_parser = StrOutputParser()\n",
    "simple_string_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "\n",
    "# Combine the simple string template with the model\n",
    "simple_string_chain = simple_string_template | gemini_model | simple_string_parser\n",
    "simple_string_response = simple_string_chain.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(f\"Simple String Output: {simple_string_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "076fbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Chain Response: Of course. Here is a brief description of a black hole:\n",
      "\n",
      "A **black hole** is a region in spacetime where gravity is so incredibly strong that nothing‚Äînot even light‚Äîcan escape from it.\n",
      "\n",
      "Here are the key points in brief:\n",
      "\n",
      "*   **How They Form:** Most black holes are formed from the remnants of a massive star that has died in a supernova explosion. The star's core collapses under its own immense gravity, squeezing an enormous amount of mass into an infinitesimally small space.\n",
      "\n",
      "*   **Key Features:**\n",
      "    *   **Singularity:** At the center is a point of infinite density called the singularity, where the known laws of physics break down.\n",
      "    *   **Event Horizon:** This is the boundary around the black hole known as the \"point of no return.\" Once anything crosses the event horizon, it is trapped forever and cannot escape.\n",
      "\n",
      "*   **How They Work:** A black hole's gravity is so powerful because it has a huge amount of mass concentrated in a tiny volume. They are not cosmic vacuum cleaners; they follow the same laws of gravity as other objects. You have to get very close to the event horizon to be pulled in. If our Sun were replaced by a black hole of the same mass, Earth would continue to orbit it just as it does now.\n",
      "\n",
      "*   **Effects on Spacetime:** As predicted by Einstein's theory of general relativity, their immense mass dramatically warps the fabric of spacetime around them. This can cause extreme effects like **time dilation**, where time slows down for an observer closer to the black hole.\n",
      "\n",
      "In essence, a black hole is the ultimate expression of gravity, representing one of the most extreme and mysterious objects in the universe.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chains\n",
    "\n",
    "# Simple Chain\n",
    "simple_prompt = PromptTemplate(\n",
    "    template = \"Describe about the topic in brief: {topic}\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "simple_chain = simple_prompt | gemini_model | parser\n",
    "simple_chain_response = simple_chain.invoke({\"topic\": \"Blackhole\"})\n",
    "print(f\"Simple Chain Response: {simple_chain_response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9423904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ‡§µ‡§∞‡•ç‡§∑‡•Ä‡§Ø ‡§Ö‡§≠‡§ø‡§ú‡•Ä‡§§ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§£‡§æ, RAG ‡§è‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§∂‡§® ‡§î‡§∞ ‡§è‡§ú‡•á‡§Ç‡§ü‡§ø‡§ï ‡§è‡§Ü‡§à ‡§Æ‡•â‡§°‡§≤ ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§≤‡•à‡§Ç‡§ó‡§ö‡•á‡§® (Langchain) ‡§î‡§∞ ‡§≤‡•à‡§Ç‡§ó‡§ó‡•ç‡§∞‡§æ‡§´ (Langgraph) ‡§∏‡•Ä‡§ñ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chain\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "translate_prompt = PromptTemplate(\n",
    "    template=\"Translate the following text to Hindi: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "sequential_chain = (\n",
    "    summary_prompt | gemini_model | StrOutputParser()\n",
    ") | (\n",
    "    translate_prompt | gemini_model | StrOutputParser()\n",
    ")\n",
    "\n",
    "sequential_chain_response = sequential_chain.invoke({\n",
    "    \"text\": \"Hello My name is Abhijit Maharana and I am 22 years old. I am learning about Langchain and Langgrpah to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(sequential_chain_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c086d792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Parallel Chain Response: Summary: Abhijit Maharana, a 22-year-old, is learning Langchain and Langgraph to develop RAG applications and Agentic AI models.\n",
      "\n",
      "Questions:\n",
      "**Question:** What is Abhijit Maharana's age?\n",
      "**Answer:** Abhijit Maharana is 22 years old.\n",
      "\n",
      "**Question:** What technologies is Abhijit Maharana learning to use?\n",
      "**Answer:** Abhijit Maharana is learning Langchain and Langgraph.\n",
      "\n",
      "**Question:** What type of applications is Abhijit Maharana aiming to build using these technologies?\n",
      "**Answer:** Abhijit Maharana is building RAG applications and Agentic AI models.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Parallel Chain\n",
    "question_prompt = PromptTemplate(\n",
    "    template=\"\"\"Make 3 questions and answers from the given text: {text}\n",
    "    Format: \n",
    "    Question: \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize the following text: {text}\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "merge_prompt = PromptTemplate(\n",
    "    template=\"\"\"Merge the following summary and questions into a single response.\n",
    "    Format should be like this:\n",
    "    Summary: {summary}\n",
    "    Questions: {questions}\n",
    "    \"\"\",\n",
    "    input_variables=[\"summary\", \"questions\"]\n",
    ")\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"summary\": summary_prompt | gemini_model | StrOutputParser(),\n",
    "        \"questions\": question_prompt | gemini_model | StrOutputParser()\n",
    "    }\n",
    ")\n",
    "\n",
    "merge_chain = merge_prompt | nvidia_model | StrOutputParser()\n",
    "\n",
    "final_parallel_chain = parallel_chain | merge_chain\n",
    "final_parallel_response = final_parallel_chain.invoke({\n",
    "    'text': \"Hello, I am Abhijit Maharana. I am 22 years old and I am learning about Langchain and Langgraph to build RAG applications and Agentic AI models.\"\n",
    "})\n",
    "\n",
    "print(f\"Final Parallel Chain Response: {final_parallel_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dca978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feedback Response: We're so sorry to hear you had a negative experience with our product and that you found the camera unsatisfactory.  We value your feedback and want to understand what specifically didn't meet your expectations. Could you please tell us more about what you found lacking in the camera's performance?  This will help us improve our products in the future.  We'd also like to explore options to help resolve this for you. Please contact us directly at [phone number or email address] so we can assist you further.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, Literal\n",
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "# Conditional Chain\n",
    "class ProductReview(BaseModel):\n",
    "    product_feedback_from_user: Annotated[str, Field(description=\"Feedback provided by the user about the product\")]\n",
    "    sentiment_of_feedback: Annotated[Literal['Positive', 'Negative'], Field(description=\"Sentiment of the feedback provided by the user\")]\n",
    "\n",
    "string_parser = StrOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ProductReview)\n",
    "    \n",
    "sentiment_analysis_prompt = PromptTemplate(\n",
    "    template = \"\"\"Analyze the sentiment of the following feedback: \n",
    "    {feedback}\n",
    "    \n",
    "    Format instructions: {format_instructions}\"\"\",\n",
    "    input_variables=[\"feedback\"],\n",
    "    partial_variables={\"format_instructions\": pydantic_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "positive_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for positive feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "negative_feedback_prompt = PromptTemplate(\n",
    "    template = \"Give a polite reply for negative feedback of the customer response for the following feedback: {feedback}\",\n",
    "    input_variables=[\"feedback\"]\n",
    ")\n",
    "\n",
    "sentiment_chain = sentiment_analysis_prompt | gemini_model | pydantic_parser\n",
    "\n",
    "feedback_chain = RunnableBranch(\n",
    "    (lambda x: x.sentiment_of_feedback == \"Positive\", positive_feedback_prompt | gemini_model | string_parser),\n",
    "    (lambda x: x.sentiment_of_feedback == \"Negative\", negative_feedback_prompt | gemini_model | string_parser),\n",
    "    RunnableLambda(lambda x: \"404 Error\")\n",
    ")\n",
    "\n",
    "final_feedback_chain = sentiment_chain | feedback_chain\n",
    "final_feedback_response = final_feedback_chain.invoke({\n",
    "    \"feedback\": \"The product very bad. Camera is not good at all!\"\n",
    "})\n",
    "\n",
    "print(f\"Final Feedback Response: {final_feedback_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f38bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
